{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ywqjsyHz0E3k",
        "outputId": "96420941-5b90-44f4-efc1-27ac09b69060"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f_1ROtXmlRtm"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, regularizers\n",
        "import xgboost as xgb\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This imports essential libraries for file handling, numerical operations, data manipulation, image processing, deep learning, and model building"
      ],
      "metadata": {
        "id": "-VrV3-fBUE_-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define paths\n",
        "csv_file_path = '/content/drive/MyDrive/hackathon/train_data.csv'\n",
        "test_csv_file_path = '/content/drive/MyDrive/hackathon/test_data_no_target.csv'\n",
        "submission_template_path = '/content/drive/MyDrive/hackathon/submission_template.csv'\n",
        "train_img_folder = '/content/drive/MyDrive/images_train/'\n",
        "test_img_folder = '/content/drive/MyDrive/images_test/'"
      ],
      "metadata": {
        "id": "eNfOJa-AQ6x-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "These paths specify locations for the training and test CSV files, a submission template, and folders containing the training and test images."
      ],
      "metadata": {
        "id": "CVeqDNRnUIjP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load training CSV data\n",
        "train_data = pd.read_csv(csv_file_path)\n",
        "train_data['ID'] = train_data['ID'].astype(int)\n",
        "\n",
        "# Load test CSV data (without price)\n",
        "test_data = pd.read_csv(test_csv_file_path)\n",
        "test_data['ID'] = test_data['ID'].astype(int)\n"
      ],
      "metadata": {
        "id": "A-AhAGCxRAkL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, training and test CSV files are loaded into DataFrames, and the ID column is cast to an integer data type for consistency. When we first started it would convert the ID into a float and not be accessible."
      ],
      "metadata": {
        "id": "AOTiXFRrUNH6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize the training target prices at the beginning\n",
        "max_price = train_data['Price'].max()\n",
        "train_data['Price'] = train_data['Price'] / max_price  # Normalize training target\n",
        "\n",
        "\n",
        "# Normalize other features in the training and test sets (using the training max)\n",
        "for feature in ['Bedrooms', 'Bathrooms', 'Area', 'ZipCode']:\n",
        "    max_val = train_data[feature].max()\n",
        "    train_data[feature] = train_data[feature] / max_val\n",
        "    test_data[feature] = test_data[feature] / max_val\n"
      ],
      "metadata": {
        "id": "AOLx3bFsRKBk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This normalizes the Price field in the training set file and scales the other features in both training and test data based on their maximum values in the training set."
      ],
      "metadata": {
        "id": "i4nsSF-IUfPL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Image size for model input\n",
        "img_size = (32, 32)\n",
        "train_image_paths = {}\n",
        "test_image_paths = {}\n",
        "\n",
        "\n",
        "# Create dictionaries with image paths for each house ID in training and test sets\n",
        "def populate_image_paths(data_split, image_folder, image_paths):\n",
        "    for _, row in data_split.iterrows():\n",
        "        image_id = int(row['ID'])\n",
        "        for room in ['front', 'bedroom', 'kitchen', 'bathroom']:\n",
        "            image_name = f\"{image_id}_{room}.jpg\"\n",
        "            full_path = os.path.join(image_folder, image_name)\n",
        "            if os.path.exists(full_path):\n",
        "                if image_id not in image_paths:\n",
        "                    image_paths[image_id] = []\n",
        "                image_paths[image_id].append(full_path)\n",
        "\n",
        "\n",
        "populate_image_paths(train_data, train_img_folder, train_image_paths)\n",
        "populate_image_paths(test_data, test_img_folder, test_image_paths)"
      ],
      "metadata": {
        "id": "nMEDie-8RNr9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This block sets the desired image size for CNN input and initializes dictionaries to store image paths for each ID in the training and test sets. This part was changed multiple times as we realised that having a large image size meant the program would be slower as the computational cost was higher."
      ],
      "metadata": {
        "id": "8o4NSU2iUq_E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate images and labels for train set\n",
        "def load_image_data(data_split, image_paths):\n",
        "    image_data, labels = [], []\n",
        "    for _, row in data_split.iterrows():\n",
        "        image_id = int(row['ID'])\n",
        "        if image_id in image_paths:\n",
        "            img = Image.open(image_paths[image_id][0]).convert('RGB')\n",
        "            img = img.resize(img_size)\n",
        "            img_array = np.array(img) / 255.0\n",
        "            image_data.append(img_array)\n",
        "            labels.append(row['Price'])\n",
        "    return np.array(image_data), np.array(labels)\n",
        "\n",
        "\n",
        "# Load train data\n",
        "X_train_images, y_train = load_image_data(train_data, train_image_paths)\n",
        "\n",
        "\n",
        "# Load only images for test data (no labels)\n",
        "def load_image_data_no_labels(data_split, image_paths):\n",
        "    image_data, ids = [], []\n",
        "    for _, row in data_split.iterrows():\n",
        "        image_id = int(row['ID'])\n",
        "        if image_id in image_paths:\n",
        "            img = Image.open(image_paths[image_id][0]).convert('RGB')\n",
        "            img = img.resize(img_size)\n",
        "            img_array = np.array(img) / 255.0\n",
        "            image_data.append(img_array)\n",
        "            ids.append(image_id)\n",
        "    return np.array(image_data), ids\n",
        "\n",
        "\n",
        "# Load test data (only images and IDs)\n",
        "X_test_images, test_ids = load_image_data_no_labels(test_data, test_image_paths)\n"
      ],
      "metadata": {
        "id": "cjyupurdRvWu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function goes through each entry in the dataset, matches images with room names to IDs, and populates the dictionary with paths for each image if the file exists. It also loads and resizes each image, normalizes pixel values, and stores them along with labels (prices) in separate lists, returning them as NumPy arrays."
      ],
      "metadata": {
        "id": "KGRy32TOVfJv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Efficient loading and augmentation using ImageDataGenerator\n",
        "train_datagen = ImageDataGenerator(horizontal_flip=True, vertical_flip=True, rotation_range=20)\n",
        "train_generator = train_datagen.flow(X_train_images, y_train, batch_size=32)\n"
      ],
      "metadata": {
        "id": "ye0C8KYbR-7T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TensorFlow's ImageDataGenerator helps us create augmented versions of our training images by applying transformations like flipping and rotating, making our model more robust. In our project, we used it to generate batches of these varied images during training, which improved our model's ability to generalize and reduced the risk of overfitting."
      ],
      "metadata": {
        "id": "Ngzoo7ANWHyq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a custom CNN model architecture suitable for 32x32 input\n",
        "class SimpleCNN(tf.keras.Model):\n",
        "    def __init__(self, weight_decay=0.01):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        self.conv1 = layers.Conv2D(32, (3, 3), activation='relu', padding='same', kernel_regularizer=regularizers.l2(weight_decay))\n",
        "        self.pool1 = layers.MaxPooling2D((2, 2))\n",
        "        self.conv2 = layers.Conv2D(64, (3, 3), activation='relu', padding='same', kernel_regularizer=regularizers.l2(weight_decay))\n",
        "        self.pool2 = layers.MaxPooling2D((2, 2))\n",
        "        self.flatten = layers.Flatten()\n",
        "        self.dense1 = layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l2(weight_decay))\n",
        "        self.dropout = layers.Dropout(0.5)\n",
        "        self.output_layer = layers.Dense(1)  # Output layer with linear activation by default\n",
        "\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.conv1(inputs)\n",
        "        x = self.pool1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.pool2(x)\n",
        "        x = self.flatten(x)\n",
        "        x = self.dense1(x)\n",
        "        x = self.dropout(x)\n",
        "        return self.output_layer(x)\n"
      ],
      "metadata": {
        "id": "4n2hTZBNSElT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This block defines a CNN model class with convolutional, pooling, and dense layers, designed to handle images of size 32x32. We also tried to handle images of size 64x64 but it was too large. Initially, we tried using many layers in our CNN, thinking it would improve accuracy, but that actually hurt our model's performance and led to overfitting. So, we simplified the architecture by reducing the number of layers and added a dropout layer to help regularize it. We chose the ReLU activation function because it outperformed leaky ReLU in our tests; ReLU helps the model learn faster by allowing only positive values to pass through while turning negative inputs into zero, which helps prevent issues like vanishing gradients."
      ],
      "metadata": {
        "id": "oAOKWYEzWsxg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom fit function\n",
        "def custom_fit(model, train_generator, epochs):\n",
        "    model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
        "    model.fit(train_generator, epochs=epochs)\n",
        "\n",
        "\n",
        "# Build and fit the CNN model\n",
        "cnn_model = SimpleCNN()\n",
        "custom_fit(cnn_model, train_generator, epochs=50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BEw609LkSQhf",
        "outputId": "7df31e31-511f-4b6c-abc3-23adf3fab83d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 57ms/step - loss: 2.6846 - mae: 0.2691\n",
            "Epoch 2/50\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - loss: 1.1874 - mae: 0.0613\n",
            "Epoch 3/50\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 87ms/step - loss: 0.6512 - mae: 0.0557\n",
            "Epoch 4/50\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 99ms/step - loss: 0.4466 - mae: 0.0526\n",
            "Epoch 5/50\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - loss: 0.3492 - mae: 0.0586\n",
            "Epoch 6/50\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - loss: 0.2811 - mae: 0.0554\n",
            "Epoch 7/50\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - loss: 0.2353 - mae: 0.0589\n",
            "Epoch 8/50\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - loss: 0.1988 - mae: 0.0519\n",
            "Epoch 9/50\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - loss: 0.1724 - mae: 0.0559\n",
            "Epoch 10/50\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - loss: 0.1517 - mae: 0.0532\n",
            "Epoch 11/50\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - loss: 0.1355 - mae: 0.0535\n",
            "Epoch 12/50\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - loss: 0.1206 - mae: 0.0565\n",
            "Epoch 13/50\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - loss: 0.1087 - mae: 0.0530\n",
            "Epoch 14/50\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 94ms/step - loss: 0.0998 - mae: 0.0582\n",
            "Epoch 15/50\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 101ms/step - loss: 0.0908 - mae: 0.0569\n",
            "Epoch 16/50\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 57ms/step - loss: 0.0852 - mae: 0.0586\n",
            "Epoch 17/50\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - loss: 0.0736 - mae: 0.0569\n",
            "Epoch 18/50\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - loss: 0.0669 - mae: 0.0548\n",
            "Epoch 19/50\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - loss: 0.0621 - mae: 0.0555\n",
            "Epoch 20/50\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - loss: 0.0569 - mae: 0.0523\n",
            "Epoch 21/50\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - loss: 0.0553 - mae: 0.0559\n",
            "Epoch 22/50\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - loss: 0.0533 - mae: 0.0593\n",
            "Epoch 23/50\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - loss: 0.0454 - mae: 0.0533\n",
            "Epoch 24/50\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - loss: 0.0421 - mae: 0.0494\n",
            "Epoch 25/50\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - loss: 0.0398 - mae: 0.0562\n",
            "Epoch 26/50\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - loss: 0.0389 - mae: 0.0545\n",
            "Epoch 27/50\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 96ms/step - loss: 0.0356 - mae: 0.0567\n",
            "Epoch 28/50\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 54ms/step - loss: 0.0331 - mae: 0.0499\n",
            "Epoch 29/50\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - loss: 0.0335 - mae: 0.0572\n",
            "Epoch 30/50\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - loss: 0.0291 - mae: 0.0529\n",
            "Epoch 31/50\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 53ms/step - loss: 0.0277 - mae: 0.0525\n",
            "Epoch 32/50\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - loss: 0.0262 - mae: 0.0557\n",
            "Epoch 33/50\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 53ms/step - loss: 0.0293 - mae: 0.0577\n",
            "Epoch 34/50\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 53ms/step - loss: 0.0231 - mae: 0.0509\n",
            "Epoch 35/50\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - loss: 0.0242 - mae: 0.0573\n",
            "Epoch 36/50\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - loss: 0.0236 - mae: 0.0587\n",
            "Epoch 37/50\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 93ms/step - loss: 0.0241 - mae: 0.0570\n",
            "Epoch 38/50\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 99ms/step - loss: 0.0213 - mae: 0.0577\n",
            "Epoch 39/50\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 55ms/step - loss: 0.0198 - mae: 0.0527\n",
            "Epoch 40/50\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - loss: 0.0192 - mae: 0.0563\n",
            "Epoch 41/50\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - loss: 0.0203 - mae: 0.0595\n",
            "Epoch 42/50\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - loss: 0.0171 - mae: 0.0532\n",
            "Epoch 43/50\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 60ms/step - loss: 0.0177 - mae: 0.0566\n",
            "Epoch 44/50\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - loss: 0.0174 - mae: 0.0570\n",
            "Epoch 45/50\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - loss: 0.0152 - mae: 0.0544\n",
            "Epoch 46/50\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - loss: 0.0158 - mae: 0.0568\n",
            "Epoch 47/50\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 93ms/step - loss: 0.0156 - mae: 0.0560\n",
            "Epoch 48/50\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 93ms/step - loss: 0.0153 - mae: 0.0557\n",
            "Epoch 49/50\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 54ms/step - loss: 0.0117 - mae: 0.0492\n",
            "Epoch 50/50\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - loss: 0.0190 - mae: 0.0617\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, a custom fit function is defined, which compiles and trains the CNN model using mean squared error as the loss function. We wanted to use a root mean sqaured error but it did not really matter at that stage. We initially experimented with the Adam optimizer due to its adaptive learning rate capabilities, which often lead to faster convergence in training neural networks. Afterward, we transitioned to Wadam, an enhancement of Adam that incorporates weight decay, aiming to improve generalization. Ultimately, we found that reverting to the Adam optimizer yielded the best results, likely due to its robust performance across various training scenarios and its ability to handle noisy gradients effectively. We used to use CosineAnnealingScheduler to adjust the learning rate at the beginning of each epoch but using it with adam made the program overcomplicated."
      ],
      "metadata": {
        "id": "oyHa1PxEW2VX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract features from the training images using the CNN\n",
        "X_train_features = cnn_model.predict(X_train_images)\n",
        "\n",
        "\n",
        "# Extract features from the test images using the CNN\n",
        "X_test_features = cnn_model.predict(X_test_images)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VPq9cstRSWfT",
        "outputId": "62dc4ade-87ec-48f6-9fd3-efb47fa2f8b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Combining CNN features with other tabular features, the final training and test feature sets for XGBoost are created."
      ],
      "metadata": {
        "id": "XvJeTAEOXES9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train XGBoost model\n",
        "xgb_model = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100)\n",
        "xgb_model.fit(X_train_final, y_train)\n",
        "# Make predictions on the test set using XGBoost\n",
        "y_pred_normalized = xgb_model.predict(X_test_final)"
      ],
      "metadata": {
        "id": "WtWxOdU6SzKv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This block initializes, trains the XGBoost model, and generates predictions for the test set in normalized form. XGBoost was a strong choice for improving model accuracy because it builds on gradient boosting with techniques that help prevent overfitting, like reducing unnecessary complexity in the model. Before XGBoost, we tried using effiecientNet to boost the model’s accuracy, but it ended up overfitting, meaning it performed well on training data but poorly on new data."
      ],
      "metadata": {
        "id": "ezYzaB08XH42"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert normalized predictions back to actual prices\n",
        "y_pred_actual = y_pred_normalized * max_price\n",
        "\n",
        "\n",
        "# Load the submission template\n",
        "submission_template = pd.read_csv(submission_template_path)\n",
        "\n",
        "\n",
        "# Fill in the predicted prices (actual) based on test IDs\n",
        "for idx, id_val in enumerate(test_ids):\n",
        "    submission_template.loc[submission_template['ID'] == id_val, 'Predicted_Price'] = y_pred_actual[idx]\n",
        "\n",
        "\n",
        "# Save predictions to the submission template\n",
        "submission_template.to_csv(submission_template_path, index=False)\n",
        "\n",
        "\n",
        "print(\"Predictions saved to submission_template.csv.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CLgDAnrBTJmc",
        "outputId": "ddb0907c-9d0b-41da-d66c-a5d4dd84620e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions saved to submission_template.csv.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This block converts normalized predictions back to actual prices by multiplying each prediction by max_price, yielding y_pred_actual. It then updates submission_template.csv with these actual price predictions for each test ID, saving the final file for submission and confirming completion."
      ],
      "metadata": {
        "id": "rzrbhcEdXLuj"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}